---
title: Text scraping Styleforum’s “Random Fashion Thoughts” thread
author: Allen Chang
date: '2018-01-08'
tags:
  - R
  - textscraping
  - visualization
slug: text-scraping-styleforum-s-random-fashion-thoughts-thread
---



<p>Styleforum’s “Random fashion thoughts” is one of the most popular sections of the website. It’s a thread where users can talk about almost anything they want regarding clothing and fashion. Every once in awhile the thread gets so long that the moderators opt to close it off and create a new post. The most recent iteration, RFT II, was recently retired so I decided to give myself a weekend project and run some text analysis on it.</p>
<p>As usual, here’s the packages I used.</p>
<pre class="r"><code>library(rvest)
library(tidyverse)
library(stringr)
library(tm)
library(wordcloud)</code></pre>
<section id="scraping-text-data-from-styleforum" class="level3">
<h3>Scraping text data from Styleforum</h3>
<p>The first thing I needed to do was scrape some information. Luckily, the package “rvest” and the SelectorGadget web extension made scraping easy. I simply used SelectorGadget to identify the css that I wanted and passed the information to a few functions from rvest. For this first part, I pulled username information.</p>
<pre class="r"><code># Set up website to scrape text from
theme_set(theme_bw())

url &lt;- &quot;https://www.styleforum.net/threads/random-fashion-thoughts-part-ii-a-new-hope.480819/page-&quot;
page &lt;- c(2:1551)
site &lt;- paste0(url, page)

# function to pull usernames
pull_users &lt;- function(i){
  read_html(i) %&gt;%
    html_nodes(&quot;.userText .username&quot;) %&gt;%
    html_text() %&gt;%
    as.data.frame()
}</code></pre>
<p>Then I used lapply to scrape all the usernames with posts on each page and rbind to package it into a dataframe.</p>
<pre class="r"><code># scrape and bind to dataframe
user_list &lt;- lapply(site, pull_users)
user_df &lt;- do.call(rbind, user_list)</code></pre>
</section>
<section id="visualizing-the-most-frequent-posters" class="level3">
<h3>Visualizing the most frequent posters</h3>
<p>With all my information in a handy dataframe, I counted the total number of posts by each user. I also converted my user column into a factor arranged by post count for easier plotting.</p>
<pre class="r"><code># get username counts and arrange in descending order
names(user_df) &lt;- c(&quot;user&quot;)
user_arranged &lt;- user_df %&gt;%
  group_by(user) %&gt;%
  summarise(count = n()) %&gt;%
  arrange(desc(count))

user_arranged$user &lt;- factor(user_arranged$user, levels = user_arranged$user)</code></pre>
<pre><code>## # A tibble: 6 x 2
##              user count
##            &lt;fctr&gt; &lt;int&gt;
## 1          LA Guy  1985
## 2     dieworkwear  1447
## 3         nahneun   758
## 4 OccultaVexillum   655
## 5       Find Finn   598
## 6           Fuuma   540</code></pre>
<p>Then I plotted a simple lollipop chart to see what users had the most posts.</p>
<p><img src="/posts/2018-01-07-text-scraping-styleforum-s-random-fashion-thoughts-thread_files/figure-html/post%20count%20plot-1.png" width="672" /></p>
</section>
<section id="text-mining-post-data" class="level3">
<h3>Text mining post data</h3>
<p>Moving on, I wanted to play around with analyzing the actual content of posts themselves, so I re-configured my scraping function to pull post information instead. The post information was much more complex to deal with. Part of this was due to html formatting in the posts, but there was also username information, and “quoting” formatting that I had to get rid of. To be perfectly honest, I want to be the first to say that the number of gsubs here looks particularly ugly. Hopefully, I’ll get a chance to make it more elegant down the line, but for a quick scrape it did the job.</p>
<pre class="r"><code># pull text from posts, don&#39;t include quoted text or html formatting
pull_content &lt;- function(i){
  read_html(i) %&gt;%
    html_nodes(&quot;.SelectQuoteContainer&quot;) %&gt;%
    html_text() %&gt;%
    gsub(&#39;[\r\n\t]&#39;, &#39;&#39;, .) %&gt;%
    gsub(&quot;.*:&quot;, &quot;&quot;, .) %&gt;% 
    gsub(&quot;↑.*Click to expand...&quot;, &quot;&quot;, .) %&gt;%
    gsub(&quot;//.*Click to expand...&quot;, &quot;&quot;, .) %&gt;%
    str_trim() %&gt;%
    as.data.frame()
}</code></pre>
<pre class="r"><code>content_list &lt;- lapply(site, pull_content)
content_df &lt;- do.call(rbind, content_list)</code></pre>
<p>Once I had my dataframe of posts, I was ready to do some pre-processing using the “tm” (text mining) package. I created a Source object using my dataframe and converted that object to a volatile corpus using a few tm functions.</p>
<pre class="r"><code># Convert dataframe to volatile corpus
content_df &lt;- rownames_to_column(content_df, &quot;postnum&quot;)
names(content_df) &lt;- c(&quot;doc_id&quot;, &quot;text&quot;)

content_source &lt;- DataframeSource(content_df)
content_corpus &lt;- VCorpus(content_source)</code></pre>
<p>Next I ran a series of cleaning functions to my corpus. I ran this a few times to add words that I thought should be excluded (most of which ended up being contractions).</p>
<p>Once I had my cleaned corpus, I was just a few steps away from analysis. To get my terms as rows and my documents as columns, I used the TermDocumentMatrix function. After that all that was left was to get the term frequency and store it into a dataframe.</p>
<pre class="r"><code># Convert corpus to tdm
content_tdm &lt;- TermDocumentMatrix(clean_corp)

# Convert tdm to a matrix: content_tdm
content_tm &lt;- as.matrix(content_tdm)

term_frequency &lt;- rowSums(content_tm)

# Sort term_frequency in descending order
term_frequency &lt;- sort(term_frequency, decreasing = T)

# View the top 10 most common words
term_frequency[1:10]</code></pre>
<pre><code>##  think    one people   good   much   know  stuff   will    now pretty 
##   2695   2473   2196   2014   1786   1649   1553   1542   1540   1532</code></pre>
<pre class="r"><code>post_word_freqs &lt;- data.frame(
  term = names(term_frequency),
  num = term_frequency
)</code></pre>
</section>
<section id="visualizing-text-data" class="level3">
<h3>Visualizing text data</h3>
<p>Finally some visualizations! The “wordcloud” package has an aptly named function wordcloud.</p>
<p><img src="/posts/2018-01-07-text-scraping-styleforum-s-random-fashion-thoughts-thread_files/figure-html/wordcloud%20plot-1.png" width="672" /></p>
<p>This gave me the most popular words from the thread, but I was also interested in what words are normally associated with the actual word “fashion”. Lucky for me there was a function in the “tm” package to do that as well.</p>
<pre class="r"><code># Find words correlated with the phrase &quot;fashion&quot;
associations &lt;- findAssocs(content_tdm, &quot;fashion&quot;, 0.2)
associations_df &lt;- as.data.frame(associations)
associations_df &lt;- rownames_to_column(associations_df, &quot;term&quot;)
associations_df &lt;- associations_df %&gt;%
  arrange(desc(fashion))

associations_df$term &lt;- factor(associations_df$term, levels = associations_df$term)</code></pre>
<p><img src="/posts/2018-01-07-text-scraping-styleforum-s-random-fashion-thoughts-thread_files/figure-html/association_plot-1.png" width="672" /></p>
<p>Unsurprisingly, terms like “designers” and “writers” come up fairly frequently when people talk about fashion. Somewhat more interesting though are terms like anti-sportcoat (punctuation is removed in the pre-processing) as well as workwear. All in all, this was a fun little project for me to play around with text analysis and learn more about my fellow users on styleforum. If you are interested in any of the data, I will include it on my github as well.</p>
</section>
<section id="references" class="level3">
<h3>References</h3>
<p><a href="https://stackoverflow.com/questions/30541445/r-rvest-got-hidden-text-i-dont-want">removing html formatting</a></p>
<p><a href="https://rpubs.com/williamsurles/316682">text analysis help</a></p>
<p><a href="https://www.datacamp.com/courses/intro-to-text-mining-bag-of-words">Datacamp: Text Mining</a></p>
</section>
