---
title: Web scraping the most popular fit pictures of 2017 from Styleforum’s “What
  are you wearing today” thread
author: Allen Chang
date: '2018-01-16'
tags:
  - R
  - clothing
  - textscraping
slug: web-scraping-the-2017-most-popular-fit-pictures-on-styleforum-s-what-are-you-wearing-today-thread
---



<p>One of the main draws of Styleforum is it’s “What Are You Wearing Today” thread. In it, users get to show off their clothes in pictures, other users can give “likes” to deem a fit exemplary (if you’ve ever opened up Instagram this concept is probably not too foreign for you). I wanted to see not only which posts were given the most likes in 2017, but I also wanted to download the photo(s) from each fit to upload into a photo album to share with others. The gameplan was to get scrape the data, organize posts by descending number of likes, then download the pcitures.</p>
<p>I’ll start off with the list of packages I used</p>
<pre class="r"><code>library(rvest)
library(stringr)
library(dplyr)
library(purrr)
library(ggplot2)</code></pre>
<p>Next I set up the list of pages to scrape.</p>
<pre class="r"><code>url &lt;- &quot;https://www.styleforum.net/threads/the-what-are-you-wearing-today-waywt-discussion-thread-part-ii.394687/page-&quot;
page &lt;- c(1792:2138)
site &lt;- paste0(url, page)

head(site, 3)</code></pre>
<pre><code>## [1] &quot;https://www.styleforum.net/threads/the-what-are-you-wearing-today-waywt-discussion-thread-part-ii.394687/page-1792&quot;
## [2] &quot;https://www.styleforum.net/threads/the-what-are-you-wearing-today-waywt-discussion-thread-part-ii.394687/page-1793&quot;
## [3] &quot;https://www.styleforum.net/threads/the-what-are-you-wearing-today-waywt-discussion-thread-part-ii.394687/page-1794&quot;</code></pre>
<p>To identify which CSS selectors I needed to scrape, I used <a href="http://selectorgadget.com/">selectorgadget</a>. The information I was interested in were the usernames and number of likes for each post. Because some posts didn’t have likes, I set up two functions. One to pull the list of users and the post numbers from each page, and another function to pull posts numbers with likes I also included the hypertext reference from each post to use a little later for downloading the actual images themselves.</p>
<pre class="r"><code>pull_users &lt;- function(i){
  html &lt;- read_html(i)
  post_href &lt;- html_nodes(html, &quot;.hashPermalink&quot;)
  postnum &lt;- html_attr(post_href, &quot;href&quot;) %&gt;% str_sub(., start= -7)
  postfull &lt;- html_attr(post_href, &quot;href&quot;)
  user &lt;- html_nodes(html, &quot;.userText .username&quot;) %&gt;% html_text()

  cbind(postnum, user, postfull) %&gt;% as.data.frame()
}

pull_likes &lt;- function(i){
  html &lt;- read_html(i)
    likes &lt;- html_nodes(html, &quot;.likesTotal&quot;)
    num_likes &lt;- likes %&gt;% html_text()
    postnum &lt;- html_attr(likes, &quot;href&quot;) %&gt;% str_extract(., &quot;(?&lt;=\\/)[^/]+(?=\\/)&quot;)
    
    cbind(postnum, num_likes) %&gt;% as.data.frame()
}</code></pre>
<p>Once I had the CSS information that I needed, I applied my scraping functions across all pages.</p>
<pre class="r"><code>user_list &lt;- lapply(site, pull_users)
user_df &lt;- do.call(rbind, user_list)

likes_list &lt;- lapply(site, pull_likes)
likes_df &lt;- do.call(rbind, likes_list)</code></pre>
<p>Next I needed to join my user data frame with my data frame of likes In the past I’ve had problems with R setting data as factors, so I converted the post numbers and like counts to numeric values.</p>
<pre class="r"><code>user_df$postnum &lt;- as.numeric(as.character(user_df$postnum))
likes_df$postnum &lt;- as.numeric(as.character(likes_df$postnum))
likes_df$num_likes &lt;- as.numeric(as.character(likes_df$num_likes))

waywt_df &lt;- left_join(user_df, likes_df, by = &quot;postnum&quot;) %&gt;% distinct()</code></pre>
<p>After I joined my two dataframes, I wanted to see what the distribution of likes looked like. I made a density plot and added a dashed line for the mean number of likes</p>
<p><img src="/posts/2018-01-16-web-scraping-the-2017-most-popular-fit-pictures-on-styleforum-s-what-are-you-wearing-today-thread_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<pre class="r"><code>top_posts &lt;- waywt_df %&gt;%
  filter(!is.na(num_likes)) %&gt;%
  arrange(desc(num_likes)) %&gt;%
  top_n(50, wt = num_likes) </code></pre>
<p>Here’s where things got a little more complicated. Within each post, there’s a separate reference to where the images are hosted. The first time I ran this, I ran into a problem where I was pulling image links from the entire page. To get around this, I used the CSS selector for the exact image post number.</p>
<pre class="r"><code>img &lt;- top_posts$postfull
base &lt;- &quot;https://www.styleforum.net/&quot;
img_url &lt;- paste0(base, img)

pull_imgs &lt;- function(i){
  html &lt;- read_html(i)
  # take last part of i and paste
  base &lt;- str_extract(i, &quot;#.*&quot;)
  node &lt;- paste(base, &quot;.LbImage&quot;, sep = &quot; &quot;)
  imgnode &lt;- html_nodes(html, node)
  link &lt;- imgnode %&gt;% html_attr(., &quot;src&quot;)

  link
}

img_list &lt;- lapply(img_url, pull_imgs)
img_vec &lt;- unlist(img_list)</code></pre>
<p>Now that I had a vector of links, I was ready to write them out into a text file. My plan was to use the file of links to download each image onto a local directory.</p>
<pre class="r"><code>write(img_vec, &quot;urls.txt&quot;)</code></pre>
<p>To do the actual downloading of image files, I used <a href="https://www.gnu.org/software/wget/">wget</a>. If you’re on a Mac, wget is easily installed using homebrew. To avoid pinging the server too often, I passed an argument to wget to wait 10 seconds between downloading each file. One handy line of text into my command line, and I was off downloading images.</p>
<pre class="bash"><code>wget -i urls.txt -w 10</code></pre>
<p>Once I finished downloading the images, I did a little extra processing with the Mac batch editor and some lines in UNIX to get ordered filenames. The section of code below just adds the “.jpg” extension to the name of each file (some of them downloaded without extension names).</p>
<pre class="bash"><code>for file in *; do
    if [ -f ${file} ]; then
        mv ${file} ${file}.jpg
    fi
done</code></pre>
<p>Done! The last step was to upload all the photos to an image sharing website of my choice (I prefer postimage). I’ll place the link to the album just below here. This was a handy little project for me that combined web data, regular expression practice, and even a little bit of UNIX coding to do, but the results were worth it. I can’t even imagine how boring it would be to go through 3-400 pages of posts manually. I hope you enjoy the list of photos.</p>
<p><a href="https://postimg.org/gallery/1szjk8a92/">Styleforum’s best of 2017</a></p>
<section id="references" class="level3">
<h3>References</h3>
</section>
