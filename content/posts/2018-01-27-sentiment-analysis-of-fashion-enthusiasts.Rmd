---
title: Sentiment analysis of fashion enthusiasts
author: Allen
date: '2018-01-27'
slug: sentiment-analysis-of-fashion-enthusiasts
categories: []
tags:
  - clothing
  - R
  - visualization
  - webscraping
draft: yes
---

Here's the list of packages used 

```{r echo=TRUE, message=FALSE, warning=FALSE}
library(rvest)
library(tidyverse)
library(stringr)
library(tidytext)
library(broom)
library(lubridate)
library(igraph)
library(ggraph)

theme_set(theme_bw())
```

The first step was to pull text from all posts in the Saint Laurent Paris thread. If you've read my previous analyses, this follows the same scraping procedure as I've outlined in my (random fashion thoughts post)[https://functionallydefined.netlify.com/posts/text-scraping-styleforum-s-random-fashion-thoughts-thread/] and my (what are you wearing today post)[https://functionallydefined.netlify.com/posts/web-scraping-the-2017-most-popular-fit-pictures-on-styleforum-s-what-are-you-wearing-today-thread/].

```{r eval=FALSE, message=FALSE, warning=FALSE}
url <- "https://www.styleforum.net/threads/saint-laurent-paris-official-thread.317702/page-"
page <- c(2:664)
# page <- c(664:1330)
site <- paste0(url, page)

pull_posts <- function(i){
  html <- read_html(i)
  date <- html_nodes(html, ".datePermalink .DateTime") %>% html_text()
  post <- html_nodes(html, ".SelectQuoteContainer") %>%
    html_text() %>%
    gsub('[\r\n\t]', '', .) %>%
    gsub(".*:", "", .) %>% 
    gsub("â†‘.*Click to expand...", "", .) %>%
    gsub("//.*Click to expand...", "", .) %>%
    str_trim()
  
  cbind(date, post) %>% as.data.frame()
}

post_list <- lapply(site, pull_posts)
post_df <- do.call(rbind, post_list)
```

To preprocess my text data, I used the text mining package to remove stop words. One problem I ran into was that the number of words in my document term matrix were too many to tidy into a tokenized data frame. To get around that I split my scrape into two parts. I saved each tidied, tokenized data frame into separate files and then bound them into one data frame. 

```{r eval = FALSE, message=FALSE, warning=FALSE}
names(post_df) <- c("doc_id", "text")
content_source <- DataframeSource(post_df)
content_corpus <- VCorpus(content_source)

clean_corpus <- function(corpus){
  corpus <- tm_map(corpus, stripWhitespace)
  corpus <- tm_map(corpus, removePunctuation)
  corpus <- tm_map(corpus, content_transformer(tolower))
  corpus <- tm_map(corpus, removeWords, c(stopwords("en"), "like", "just",
                                          "really", "also", "theyre",
                                          "theres", "youre", "thats",
                                          "just", "can", "get", "dont",
                                          "ive", "etc", "cant", "saint", "black"))
  return(corpus)
}

clean_corp <- clean_corpus(content_corpus)
content_dtm <- DocumentTermMatrix(clean_corp)

content_td <- tidy(content_dtm)
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
slp1 <- readRDS("/Users/AllenChang/Desktop/functionally_defined/data/slp1_td.rds")
slp2 <- readRDS("/Users/AllenChang/Desktop/functionally_defined/data/slp2_td.rds")

full_df <- bind_rows(slp1, slp2)
names(full_df) <- c("date", "term", "count")

full_df$date <- mdy(full_df$date) 
full_df <- full_df %>%
  drop_na(date)
```

Great, once I had my full dataframe of words that appeared on each date I could do some analysis. As a first exploratory measure, I wanted to see what the top 10 words were (not yet filtering for only emotional words)

```{r echo=FALSE, message=FALSE, warning=FALSE}
full_df %>% count(term) %>% arrange(desc(n)) %>% top_n(10)
```

It seems that jeans and boots are a popular topic in the thread. Next, it was time to filter my dataframe of words for sentiment. I found that all 3 lexicons for defining sentiments each have interesting properties for analysis, so I went ahead and just inner joined my data with each lexicon. For reference, the (nrc)[http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm] lexicon categorizes words into 10 different categories of sentiment, the (bing)[https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html] lexicon categorizes into positive and negative sentiment, and the (afinn)[http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010] lexicon categorizes words into positive and negative sentiment, but also scores by degree of sentiment.

```{r echo=TRUE, message=FALSE, warning=FALSE}
post_sent_bing <- full_df %>%
  inner_join(get_sentiments("bing"), by = c(term = "word"))

post_sent_nrc <- full_df %>%
  inner_join(get_sentiments("nrc"), by = c(term = "word"))

post_sent_afinn <- full_df %>%
  inner_join(get_sentiments("afinn"), by = c(term = "word"))
```

I continued with my exploratory data by asking what the most common words were by sentiment in the nrc lexicon.

```{r}
post_sent_nrc %>%
  count(term, sentiment) %>%
  arrange(desc(n))
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
post_sent_nrc %>%
  count(term, sentiment, sort = TRUE) %>%
  ungroup() %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(term = reorder(term, n)) %>%
  ggplot(aes(term, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ sentiment, scales = "free_y") + 
  labs(y = "Count",
     x = "Term") +
  coord_flip()
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
post_sent_bing %>%
  count(sentiment, term, wt = count) %>%
  ungroup() %>%
  filter(n >= 300) %>%
  mutate(n = ifelse(sentiment == "negative", -n, n)) %>%
  mutate(term = reorder(term, n)) %>%
  ggplot(aes(term, n, fill = sentiment)) +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 11.5)) +
  ylab("Term count") + xlab("Term") +
  coord_flip()
```

```{r}
hedi_leaves <- ymd("2016-04-01")

polarity_time <- post_sent_bing %>%
  mutate(bymonth = floor_date(date, unit = "month")) %>%
  group_by(bymonth, sentiment) %>%
  summarise(tot = sum(count)) %>%
  spread(sentiment, tot, fill = 0) %>%
  mutate(polarity = positive - negative)
  
polarity_time %>%
  ggplot(aes(x = bymonth, y = polarity)) +
  geom_col() +
  geom_vline(xintercept = as.numeric(hedi_leaves), lty = 2, color = "steelblue")

post_sent_nrc %>%
  filter(sentiment %in% c("positive", "negative")) %>%
  mutate(bymonth = floor_date(date, unit = "month")) %>%
  group_by(bymonth, sentiment) %>%
  summarise(tot = sum(count)) %>%
  spread(sentiment, tot, fill = 0) %>%
  mutate(polarity = positive - negative) %>%
  ggplot(aes(x = bymonth, y = polarity)) +
  geom_col() +
  geom_vline(xintercept = as.numeric(hedi_leaves), lty = 2, color = "steelblue")
```

```{r}
polarity_time %>% 
  select(bymonth, negative) %>%
  ggplot(aes(x = bymonth, y = negative)) +
  geom_col() +
  geom_vline(xintercept = as.numeric(hedi_leaves), lty = 2, color = "steelblue")
```

```{r}
polarity_time %>% 
  select(bymonth, positive) %>%
  ggplot(aes(x = bymonth, y = positive)) +
  geom_col() +
  geom_vline(xintercept = as.numeric(hedi_leaves), lty = 2, color = "steelblue")
```

```{r message=FALSE, warning=FALSE}
raw1 <- readRDS("/Users/AllenChang/Desktop/functionally_defined/data/slp_postraw1.rds")
raw1$date <- mdy(raw1$date)
raw2 <- readRDS("/Users/AllenChang/Desktop/functionally_defined/data/slp_postraw2.rds")
raw2$date <- mdy(raw2$date)

slp_raw <- bind_rows(raw1, raw2) %>%
  drop_na(date)
```

```{r message=FALSE, warning=FALSE}
slp_bigrams <- slp_raw %>%
  unnest_tokens(bigram, post, token = "ngrams", n = 2)

slp_bigrams_sep <- slp_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

slp_bigrams_filtered <- slp_bigrams_sep %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

slp_bigrams_united <- slp_bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")
```


```{r}
slp_bigrams_united %>%
  count(bigram, sort = TRUE) %>%
  top_n(10) %>%
  mutate(bigram = reorder(bigram, n)) %>%
  ggplot(aes(bigram, n)) + 
  geom_col() +
  xlab(NULL) +
  coord_flip()
```

```{r message=FALSE, warning=FALSE}
slp_g <- slp_bigrams_sep %>%
  count(word1, word2, sort = TRUE) %>%
  filter(n > 300) %>%
  graph_from_data_frame()
```


```{r fig.height=10, fig.width=10, message=FALSE, warning=FALSE}
set.seed(2017)

ggraph(slp_g, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)
```


### afinn

sentiment_contributions <- full_df %>%
  # Count by title and word
  count(term, sort = TRUE) %>%
  # Implement sentiment analysis using the "afinn" lexicon
  inner_join(get_sentiments("afinn"), by = c(term = "word")) %>%
  # Calculate a contribution for each word in each title
  mutate(contribution = score * n / sum(n)) %>%
  ungroup()

### REFERENCES

https://cran.r-project.org/web/packages/tidytext/vignettes/tidying_casting.html
[datacamp]
[datacamp]
[tidy text book]