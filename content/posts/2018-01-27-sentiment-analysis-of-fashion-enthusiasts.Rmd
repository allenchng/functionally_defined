---
title: Sentiment analysis of fashion enthusiasts
author: Allen
date: '2018-01-30'
slug: sentiment-analysis-of-fashion-enthusiasts
categories: []
tags:
  - clothing
  - R
  - visualization
  - webscraping
---

Like most hobbyists, clothing enthusiasts tend to have strong opinions. In this post I wanted to look at how these opinions changed across time using sentiment analysis and text mining. My dataset of choice was user posts from the Saint Laurent Paris thread on Styleforum. The data was comprised of text from user posts beginning in 2012 and continuing up until today. 

As usual, here's the list of packages used. 

```{r echo=TRUE, message=FALSE, warning=FALSE}
library(rvest)
library(tidyverse)
library(stringr)
library(tidytext)
library(broom)
library(lubridate)
# library(tm)
library(igraph)
library(ggraph)

theme_set(theme_bw())
```

### Scraping post information

The first step was to pull text from all posts in the [Saint Laurent Paris thread](https://www.styleforum.net/threads/saint-laurent-paris-official-thread.317702/). If you've read my previous analyses, this follows the same scraping procedure as I've outlined in my [random fashion thoughts ](https://functionallydefined.netlify.com/posts/text-scraping-styleforum-s-random-fashion-thoughts-thread/) and my [what are you wearing today](https://functionallydefined.netlify.com/posts/web-scraping-the-2017-most-popular-fit-pictures-on-styleforum-s-what-are-you-wearing-today-thread/) posts.

```{r scrape, eval=FALSE, message=FALSE, warning=FALSE}
# scrape user posts
url <- "https://www.styleforum.net/threads/saint-laurent-paris-official-thread.317702/page-"
page <- c(2:664)
# page <- c(664:1330)
site <- paste0(url, page)

pull_posts <- function(i){
  html <- read_html(i)
  date <- html_nodes(html, ".datePermalink .DateTime") %>% html_text()
  post <- html_nodes(html, ".SelectQuoteContainer") %>%
    html_text() %>%
    gsub('[\r\n\t]', '', .) %>%
    gsub(".*:", "", .) %>% 
    gsub("â†‘.*Click to expand...", "", .) %>%
    gsub("//.*Click to expand...", "", .) %>%
    str_trim()
  
  cbind(date, post) %>% as.data.frame()
}

post_list <- lapply(site, pull_posts)
post_df <- do.call(rbind, post_list)
```

To preprocess my text data, I used the text mining (tm) package to remove stop words. One problem I ran into was that the number of words in my document term matrix were too many to tidy into a tokenized data frame. To get around that I split my scrape into two parts. I saved each tidied, tokenized data frame into separate files and then bound them into one data frame (all files can be found in my [github](https://github.com/allenchng) as well). 

```{r clean corpus, eval=FALSE, message=FALSE, warning=FALSE}
# preprocess data using text mining package
names(post_df) <- c("doc_id", "text")
content_source <- DataframeSource(post_df)
content_corpus <- VCorpus(content_source)

clean_corpus <- function(corpus){
  corpus <- tm_map(corpus, stripWhitespace)
  corpus <- tm_map(corpus, removePunctuation)
  corpus <- tm_map(corpus, content_transformer(tolower))
  corpus <- tm_map(corpus, removeWords, c(stopwords("en"), "like", "just",
                                          "really", "also", "theyre",
                                          "theres", "youre", "thats",
                                          "just", "can", "get", "dont",
                                          "ive", "etc", "cant", "saint", "black"))
  return(corpus)
}

clean_corp <- clean_corpus(content_corpus)
content_dtm <- DocumentTermMatrix(clean_corp)

content_td <- tidy(content_dtm)
```

```{r bind data, echo=TRUE, message=FALSE, warning=FALSE}
slp1 <- readRDS("slp1_td.rds")
slp2 <- readRDS("slp2_td.rds")

full_df <- bind_rows(slp1, slp2)
names(full_df) <- c("date", "term", "count")

full_df$date <- mdy(full_df$date) 
full_df <- full_df %>%
  drop_na(date)
```

### Exploring post data

Great, once I had my full dataframe of words that appeared on each date I could do some analysis. As a quick first exploratory measure, I wanted to see what were the top 10 most frequently used words (not yet filtering for sentiment)

```{r top10_nonsentiment, echo=FALSE, message=FALSE, warning=FALSE}
full_df %>% count(term) %>% arrange(desc(n)) %>% top_n(10)
```

It seems that jeans and boots are a popular topic in the thread. Next, it was time to filter my dataframe of words for sentiment. In this analysis, I ended up using 2 lexicons; the nrc and bing. For reference, the [nrc](http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm) lexicon categorizes words into 10 different categories of sentiment and the [bing](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html) lexicon categorizes into positive and negative sentiment.

```{r join sentiment, echo=TRUE, message=FALSE, warning=FALSE}
# get sentiments
post_sent_bing <- full_df %>%
  inner_join(get_sentiments("bing"), by = c(term = "word"))

post_sent_nrc <- full_df %>%
  inner_join(get_sentiments("nrc"), by = c(term = "word"))
```

I continued by asking what the most common words were by sentiment in the nrc lexicon.

```{r topbysentiment, echo=FALSE, message=FALSE, warning=FALSE}
post_sent_nrc %>%
  count(term, sentiment, sort = TRUE) %>%
  ungroup() %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(term = reorder(term, n)) %>%
  ggplot(aes(term, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ sentiment, scales = "free_y") + 
  labs(y = "Count",
     x = "Term") +
  coord_flip()
```

Similarly, I looked at the most popular words by sentiment in the bing lexicon. 

```{r posneg_count, echo=FALSE, message=FALSE, warning=FALSE}
post_sent_bing %>%
  count(sentiment, term, wt = count) %>%
  ungroup() %>%
  filter(n >= 300) %>%
  mutate(n = ifelse(sentiment == "negative", -n, n)) %>%
  mutate(term = reorder(term, n)) %>%
  ggplot(aes(term, n, fill = sentiment)) +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 11.5)) +
  ylab("Term count") + xlab("Term") +
  coord_flip()
```

### Time related changes in sentiment

After exploring the data, I shifted my focus to examining how sentiment changed across time. Going into this analysis, I had a hypothesis that sentiment would be strongly positive until early 2016, when creative director Hedi Slimane departed (represented as the dashed line on the plot below) the fashion house. In the aftermath, I expected that sentiment would drop sharply. To test this hypothesis, I summed the total number of positive and negative words every month then subtracted them from one another to get the polarity. Then I plotted the polarity of each month across time. 

```{r calc polarity, echo=TRUE, message=FALSE, warning=FALSE}
# calculate montly polarity
polarity_time <- post_sent_bing %>%
  mutate(bymonth = floor_date(date, unit = "month")) %>%
  group_by(bymonth, sentiment) %>%
  summarise(tot = sum(count)) %>%
  spread(sentiment, tot, fill = 0) %>%
  mutate(polarity = positive - negative)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
hedi_leaves <- ymd("2016-04-01")
hedi_celine <- ymd("2018-01-21")
  
polarity_time %>%
  ggplot(aes(x = bymonth, y = polarity)) +
  geom_col() +
  geom_vline(xintercept = as.numeric(hedi_leaves), lty = 2, lwd = 1.25, color = "#FFC600") +
  geom_vline(xintercept = as.numeric(hedi_celine), lty = 2, lwd = 1.25, color = "#FFC600") +
    xlab("Time") + ylab("Polarity (positive - negative)") +
  ggtitle("Polarity across time")
```

Looking at the time series of polarity, a few things jump out. In the early days, polarity was slightly positive, but it wasn't until around 2015 that polarity started to spike upwards dramatically. The second sustained spike in polarity occurs in early 2016, in the months just before Hedi leaves the company. However, contrary to my expectation, sentiment remained positive in the years to come.  

Why was my prediction so off? It occurred to me that though Hedi leaving was a highly negative event, there was an equal opportunity for posters to priase his tenure, thereby increasing the number of positive words. I proceeded to plot the sum of the negative words for each month across time. Sure enough, while there was a sharp increase in the number of negative words prior to Slimane's official departure, the increase in positive words far exceeded the negative.

Edit: On January 21st, Hedi Slimane was named the creative director of Celine. I resampled the thread to see how sentiment changed. Not surprisingly, there is an increase in polarity just prior to the announcement (most likely due to rumors). The second dashed line on each plot reflects the announcement. 

```{r echo=FALSE, message=FALSE, warning=FALSE}
polarity_time %>% 
  select(bymonth, negative) %>%
  ggplot(aes(x = bymonth, y = negative)) +
  geom_col(fill = "#FF5240") +
  geom_vline(xintercept = as.numeric(hedi_leaves), lty = 2, lwd = 1.2) +
  geom_vline(xintercept = as.numeric(hedi_celine), lty = 2, lwd = 1.2) +
  xlab("Time") + ylab("Number of negative words") +
  ggtitle("Negative words across time")

polarity_time %>% 
  select(bymonth, positive) %>%
  ggplot(aes(x = bymonth, y = positive)) +
  geom_col(fill = "#37B6CE") +
  geom_vline(xintercept = as.numeric(hedi_leaves), lty = 2, lwd = 1.2) +
  geom_vline(xintercept = as.numeric(hedi_celine), lty = 2, lwd = 1.2) +
  xlab("Time") + ylab("Number of positive words") +
  ggtitle("Positive words across time")
```

So I was wrong in my prediction, but that's quite all right! 

### Networks of words

In the last section, I wanted to move past time related changes in sentiment and transition to looking at relationships between words in the thread. I was particularly interested in words that occured consecutively with one another, or bigrams. To do this, I went back and pulled raw post data that did not go through any of the corpus pre-processing.

```{r message=FALSE, warning=FALSE, include=FALSE}
raw1 <- readRDS("slp_postraw1.rds")
raw1$date <- mdy(raw1$date)
raw2 <- readRDS("slp_postraw2.rds")
raw2$date <- mdy(raw2$date)

slp_raw <- bind_rows(raw1, raw2) %>%
  drop_na(date)
```

I relied heavily on Julia Silge's tidy text mining chapter on n-grams (click [here](https://www.tidytextmining.com/ngrams.html) for more!) to pre-process my data once again. I unnested each post, separated and filtered out stop words, then united the bigrams back together. 

```{r echo=TRUE, message=FALSE, warning=FALSE}
slp_bigrams <- slp_raw %>%
  unnest_tokens(bigram, post, token = "ngrams", n = 2)

slp_bigrams_sep <- slp_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

slp_bigrams_united <- slp_bigrams_sep %>%
  unite(bigram, word1, word2, sep = " ")
```

As a form of exploratory analysis, I again counted the top 10 most popular bigrams across the thread. 

```{r echo=FALSE, message=FALSE, warning=FALSE}
slp_bigrams_united %>%
  count(bigram, sort = TRUE) %>%
  top_n(10) %>%
  mutate(bigram = reorder(bigram, n)) %>%
  ggplot(aes(bigram, n)) + 
  geom_col(fill = "steelblue") +
  xlab(NULL) +
  coord_flip()
```

Again taking a cue from Julia's book, I plotted the network of bigrams using igraph and ggraph. Looking through, the approach does a nice job of capturing connections between words. That size cluster on the left size is especially interesting to see. 

```{r echo=TRUE, message=FALSE, warning=FALSE}
slp_g <- slp_bigrams_sep %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  count(word1, word2, sort = TRUE) %>%
  filter(n > 75) %>%
  graph_from_data_frame()
```


```{r echo=FALSE, fig.height=9, fig.width=12, message=FALSE, warning=FALSE}
set.seed(2017)

a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(slp_g, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()
```

All in all I had a lot of fun working with sentiment analysis! One of the nice things about this analysis was that it forced me to utilize a number of different techniques and methods to clean and process data. I had to use text mining and web scraping methods in conjunction with sentiment and tidy data analysis. If you're interested in doing your own sentiment analysis, I would highly recommend reading Julia Silge's book on tidy text mining and/or take her datacamp class. 

### References

[datacamp: sentiment analysis](https://www.datacamp.com/courses/sentiment-analysis-in-r)

[datacamp: tidy sentiment analysis](https://www.datacamp.com/courses/sentiment-analysis-in-r-the-tidy-way)

[tidy text book](https://www.tidytextmining.com/)