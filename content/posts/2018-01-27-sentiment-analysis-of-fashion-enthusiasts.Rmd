---
title: Sentiment analysis of fashion enthusiasts
author: Allen
date: '2018-01-27'
slug: sentiment-analysis-of-fashion-enthusiasts
categories: []
tags:
  - clothing
  - R
  - visualization
  - webscraping
draft: yes
---

I recently completed 


Here's the list of packages used.. 

```{r echo=TRUE, message=FALSE, warning=FALSE}
library(rvest)
library(tidyverse)
library(stringr)
library(tidytext)
library(broom)
library(lubridate)
library(tm)
library(igraph)
library(ggraph)

theme_set(theme_bw())
```

The first step was to pull text from all posts in the Saint Laurent Paris thread. If you've read my previous analyses, this follows the same scraping procedure as I've outlined in my [random fashion thoughts post](https://functionallydefined.netlify.com/posts/text-scraping-styleforum-s-random-fashion-thoughts-thread/) and my [what are you wearing today post](https://functionallydefined.netlify.com/posts/web-scraping-the-2017-most-popular-fit-pictures-on-styleforum-s-what-are-you-wearing-today-thread/).

```{r eval=FALSE, message=FALSE, warning=FALSE}
# scrape user posts
url <- "https://www.styleforum.net/threads/saint-laurent-paris-official-thread.317702/page-"
page <- c(2:664)
# page <- c(664:1330)
site <- paste0(url, page)

pull_posts <- function(i){
  html <- read_html(i)
  date <- html_nodes(html, ".datePermalink .DateTime") %>% html_text()
  post <- html_nodes(html, ".SelectQuoteContainer") %>%
    html_text() %>%
    gsub('[\r\n\t]', '', .) %>%
    gsub(".*:", "", .) %>% 
    gsub("â†‘.*Click to expand...", "", .) %>%
    gsub("//.*Click to expand...", "", .) %>%
    str_trim()
  
  cbind(date, post) %>% as.data.frame()
}

post_list <- lapply(site, pull_posts)
post_df <- do.call(rbind, post_list)
```

To preprocess my text data, I used the text mining package to remove stop words. One problem I ran into was that the number of words in my document term matrix were too many to tidy into a tokenized data frame. To get around that I split my scrape into two parts. I saved each tidied, tokenized data frame into separate files and then bound them into one data frame. 

```{r eval = FALSE, message=FALSE, warning=FALSE}
# preprocess data using text mining package
names(post_df) <- c("doc_id", "text")
content_source <- DataframeSource(post_df)
content_corpus <- VCorpus(content_source)

clean_corpus <- function(corpus){
  corpus <- tm_map(corpus, stripWhitespace)
  corpus <- tm_map(corpus, removePunctuation)
  corpus <- tm_map(corpus, content_transformer(tolower))
  corpus <- tm_map(corpus, removeWords, c(stopwords("en"), "like", "just",
                                          "really", "also", "theyre",
                                          "theres", "youre", "thats",
                                          "just", "can", "get", "dont",
                                          "ive", "etc", "cant", "saint", "black"))
  return(corpus)
}

clean_corp <- clean_corpus(content_corpus)
content_dtm <- DocumentTermMatrix(clean_corp)

content_td <- tidy(content_dtm)
```

```{r echo=TRUE, message=FALSE, warning=FALSE}
slp1 <- readRDS("/Users/AllenChang/Desktop/functionally_defined/data/slp1_td.rds")
slp2 <- readRDS("/Users/AllenChang/Desktop/functionally_defined/data/slp2_td.rds")

full_df <- bind_rows(slp1, slp2)
names(full_df) <- c("date", "term", "count")

full_df$date <- mdy(full_df$date) 
full_df <- full_df %>%
  drop_na(date)
```

Great, once I had my full dataframe of words that appeared on each date I could do some analysis. As a first exploratory measure, I wanted to see what the top 10 words were (not yet filtering for only emotional words)

```{r echo=FALSE, message=FALSE, warning=FALSE}
full_df %>% count(term) %>% arrange(desc(n)) %>% top_n(10)
```

It seems that jeans and boots are a popular topic in the thread. Next, it was time to filter my dataframe of words for sentiment. I found that all 3 lexicons for defining sentiments each have interesting properties for analysis, so I went ahead and just inner joined my data with each lexicon. For reference, the [nrc](http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm) lexicon categorizes words into 10 different categories of sentiment, the [bing](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html) lexicon categorizes into positive and negative sentiment, and the [afinn](http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010) lexicon categorizes words into positive and negative sentiment, but also scores by degree of sentiment.

```{r echo=TRUE, message=FALSE, warning=FALSE}
# get sentiments
post_sent_bing <- full_df %>%
  inner_join(get_sentiments("bing"), by = c(term = "word"))

post_sent_nrc <- full_df %>%
  inner_join(get_sentiments("nrc"), by = c(term = "word"))

post_sent_afinn <- full_df %>%
  inner_join(get_sentiments("afinn"), by = c(term = "word"))
```

I continued with my exploratory data by asking what the most common words were by sentiment in the nrc lexicon.

```{r echo=FALSE, message=FALSE, warning=FALSE}
post_sent_nrc %>%
  count(term, sentiment, sort = TRUE) %>%
  ungroup() %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(term = reorder(term, n)) %>%
  ggplot(aes(term, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ sentiment, scales = "free_y") + 
  labs(y = "Count",
     x = "Term") +
  coord_flip()
```

Similarly, I looked at the most popular sentiments in the bing lexicon. 

```{r echo=FALSE, message=FALSE, warning=FALSE}
post_sent_bing %>%
  count(sentiment, term, wt = count) %>%
  ungroup() %>%
  filter(n >= 300) %>%
  mutate(n = ifelse(sentiment == "negative", -n, n)) %>%
  mutate(term = reorder(term, n)) %>%
  ggplot(aes(term, n, fill = sentiment)) +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 11.5)) +
  ylab("Term count") + xlab("Term") +
  coord_flip()
```

After exploring the data, I shifted my focus to examining how sentiment changed across time. I summed the total number of positive and negative words every month then subtracted them from one another to get the polarity. 

Going into this analysis, I had a hypothesis that polarity would strongly positive until early 2016, when the head designer of SLP departed. I expected that polarity would drop sharply.

```{r echo=TRUE, message=FALSE, warning=FALSE}
# calculate montly polarity
polarity_time <- post_sent_bing %>%
  mutate(bymonth = floor_date(date, unit = "month")) %>%
  group_by(bymonth, sentiment) %>%
  summarise(tot = sum(count)) %>%
  spread(sentiment, tot, fill = 0) %>%
  mutate(polarity = positive - negative)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
hedi_leaves <- ymd("2016-04-01")
  
polarity_time %>%
  ggplot(aes(x = bymonth, y = polarity)) +
  geom_col() +
  geom_vline(xintercept = as.numeric(hedi_leaves), lty = 2, color = "steelblue")

```

Looking at the polarity changes over time, while there levels of positive polarity drop after the departure of Hedi, the overall polarity remains positive. In fact, prior to to the departure, polarity was at its highest. 

Why was my prediction so off? It occurred to me that even though Hedi Slimane leaving was a highly negative event, posters might have been singing his praises on his way out. I proceeded to plot the sum of the negative words for each month across time and sure enough, there was a sharp increase in the months prior to Slimane's official departure (I attributed this to speculation based on the rumor mill).

```{r echo=FALSE, message=FALSE, warning=FALSE}
polarity_time %>% 
  select(bymonth, negative) %>%
  ggplot(aes(x = bymonth, y = negative)) +
  geom_col() +
  geom_vline(xintercept = as.numeric(hedi_leaves), lty = 2, color = "steelblue")
```

As a contrast, I also plotted the number of positive words per month. 

```{r echo=FALSE, message=FALSE, warning=FALSE}
polarity_time %>% 
  select(bymonth, positive) %>%
  ggplot(aes(x = bymonth, y = positive)) +
  geom_col() +
  geom_vline(xintercept = as.numeric(hedi_leaves), lty = 2, color = "steelblue")
```

In the last section, I wanted to move past time related changes in sentiment and transition to looking at words that occured consecutively with one another. I went back and pulled raw post data that did not go through any of the corpus pre-processing. 

```{r message=FALSE, warning=FALSE, include=FALSE}
raw1 <- readRDS("/Users/AllenChang/Desktop/functionally_defined/data/slp_postraw1.rds")
raw1$date <- mdy(raw1$date)
raw2 <- readRDS("/Users/AllenChang/Desktop/functionally_defined/data/slp_postraw2.rds")
raw2$date <- mdy(raw2$date)

slp_raw <- bind_rows(raw1, raw2) %>%
  drop_na(date)
```

I relied heavily on Julia Silge's tidy text mining chapter on n-grams (click [here](https://www.tidytextmining.com/ngrams.html) for more!) to pre-process my data once again. I unnested each post, separated and filtered out stop words, then united the bigrams back together. 

```{r echo=TRUE, message=FALSE, warning=FALSE}
slp_bigrams <- slp_raw %>%
  unnest_tokens(bigram, post, token = "ngrams", n = 2)

slp_bigrams_sep <- slp_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

slp_bigrams_united <- slp_bigrams_sep %>%
  unite(bigram, word1, word2, sep = " ")
```

As a form of exploratory analysis, I again  counted the top 10 most popular bigrams across the thread. 

```{r echo=FALSE, message=FALSE, warning=FALSE}
slp_bigrams_united %>%
  count(bigram, sort = TRUE) %>%
  top_n(10) %>%
  mutate(bigram = reorder(bigram, n)) %>%
  ggplot(aes(bigram, n)) + 
  geom_col() +
  xlab(NULL) +
  coord_flip()
```

Again taking a cue from Julia's book, I plotted the network of bigrams using igraph and ggraph. I found it particularly amusing that that the words "skin" and "tight" are connected as are the words "knee"" and "rips". 

```{r echo=TRUE, message=FALSE, warning=FALSE}
slp_g <- slp_bigrams_sep %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  count(word1, word2, sort = TRUE) %>%
  filter(n > 75) %>%
  graph_from_data_frame()
```


```{r echo=FALSE, fig.height=9, fig.width=12, message=FALSE, warning=FALSE}
set.seed(2017)

a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(slp_g, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()
```



### REFERENCES

https://cran.r-project.org/web/packages/tidytext/vignettes/tidying_casting.html
[datacamp](https://www.datacamp.com/courses/sentiment-analysis-in-r)
[datacamp](https://www.datacamp.com/courses/sentiment-analysis-in-r-the-tidy-way)
[tidy text book](https://www.tidytextmining.com/)